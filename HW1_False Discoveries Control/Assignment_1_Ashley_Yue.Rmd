---
title: "Assignment_1_Ashley_Yue"
author: "Ashley"
date: "2023-01-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. [5 pts] Write code that produces a 10,000 x 1001 matrix (rows x cols) of random numbers drawn from N(0,1). Seed your code using the last 4 digits of your phone number (this number will be different for everyone).  Every time you run the code, it should now yield the exact same (“random”) dataset.
```{r}
# set the seed 
set.seed(9070)

# create the 10000 x 1001 matrix drawn from N(0,1)
data <- matrix(rnorm(10000 * 1001), nrow = 10000)
dim(data)
```


## 2. [5 pts] Treat the first column as “y” and the remaining 1000 columns as x’s.
```{r}
# extract the first column as "y"
y <- data[, 1]

# extract the rest columns as "x's"
x <- data[,2:1001]
```


## 3. [15 pts] Regress y on x’s. Is an intercept needed?  Why?  Why not?
```{r}
# No intercept needed because y and x's are all white noise.
# E(X) and E(Y) are all zero and a linear regression model must go through (E(X), E(Y)) which is (0,0), the origin. Therefore, there is no need for an intercept.

model_1 <- lm(y ~ x - 1)
```


## 4. [5 pts] Create a histogram of the p-values from the regression in Q3. What distribution does this histogram look like?
```{r}
# extract p-values from the linear regression model
p_values <- summary(model_1)$coefficient[, 4]
hist(p_values)
```

The histogram looks like a uniform distribution.


```{r}
ks.test(p_values,"punif")
```

The p-value of ks test is 0.2538, which is greater than 0.05. Therefore, the formal test also implies a uniform distribution of the p-values.


## 5. [15 pts] How many “significant” variables do you expect to find knowing how the data was generated? How many “significant” variables does the regression yield if alpha = 0.01?  What does this tell us?
Theoretically speaking, based on how the data was generated, I expect 0 significant variable because it's a regression of white noise on white noise. Since the data was totally randomly generated, I expect there will be 1000*alpha significant variables, i.e. if alpha = 0.01 then there will expect to be 10 significant variables. 
```{r}
# count the p-values that are less than 0.01
sum(p_values < 0.01)
```
There are actually 12 significant variables, which is close to expected number of 10. This tells us that when involving massive variables in the model, using the normal alpha, there will inevitably be a considerable amount of falsely predicted "meaningful" variables. And as the number of variables goes up, the number of false positives increase correspondingly. Therefore, when including enormous amount of variables in a model, we need to choose a much much smaller alpha for false discovery rate control.


## 6. [10 pts] Given the p values you find, use the BH procedure to control the FDR with a q of 0.1. How many “true” discoveries do you estimate?
```{r}
# sort the p-values ascendingly
pv_sorted <- sort(p_values)

# create a rank
k <- c(1:1000)

# calculate BH equation
q = 0.1
N = 1000

BH <- (pv_sorted <= q * k / N)
sum(BH)
```

None of the p-values could be the cut-off p* that control FDR <= q. So with a q of 0.1, none of the variables can be regarded as "true" discoveries even though there appears to be 12 significant variables.


## 7. [5 pts] Explore the “autos.csv” data. Include any metrics and / or plots you find interesting.
```{r}
# clear the environment
rm(list = ls())

# import the data
library(readr)
autos <- read_csv("C:/UCD/__WINTER SESSION/452 Machine Learning/Assignment 1/autos.csv")
```

```{r}
hist(autos$price)
```

The distribution of price is highly left skewed. Most prices are lower than $20000.


```{r}
plot(autos$width, autos$length)
abline(lm(autos$length ~ autos$width))
```

There is a positive correlation between the length and the width of a car.


```{r}
library(ggplot2)
ggplot(data = autos, mapping = aes(x = fuel_type, y = price)) + geom_boxplot()
```
The prices of automobiles that use diesel are on average higher than the ones of automobiles that burn gas. It may be because that diesel-burning cars are generally larger than gas-burning. However, there is a considerable amount of outliers in the gas-burning category whose prices are much higher. The fact that luxury automobiles often burn gas may account for the phenomenon. 


## 8. [15 pts] Create a linear regression model to predict price. Explain your model.
```{r}
# create a linear regression model that include all the variables in the data set because each of them can reasonably influence the price of a car
# There are multiple qualitative variables as well as quantitative variables, R would generate dummies for qualitative variables automatically

model_2 <- lm(price ~ ., data = autos)

summary(model_2)
```


## 9. [10 pts] Why might false discoveries be an issue?
There are 53 valid coefficients (excluded the two singular estimation). If 20 or more of them are actually useless, applying 0.05 as the significance level, there is expected to be 1 or more false positive variable. And based on the results, there are 30 variables appear insignificant. So it is valid to consider false discoveries as an issue here.


## 10. [15 pts] Use the BH procedure to control the FDR with a q of 0.1. How many true discoveries do you estimate? Plot the cutoff line together with the significant and insignificant p-values.
```{r}
# extract the p-values
p_values <- summary(model_2)$coefficient[,4]

# sort the p-values ascendingly
pv_sorted <- sort(p_values)

# create the rank
k <- c(1:53)

# calculate BH equation
q <- 0.1
N <- length(k)

BH <- pv_sorted <= k*q/N

# count the estimated true discoveries
sum(BH)
```

The number of true discoveries is estimated to be 19.


```{r}
# calculate the cut-off p-value (p*)
p_star <- pv_sorted[sum(BH)]
sprintf("The cut-off p-value is %.5f.", p_star)
```

```{r}
# Plot the cutoff line together with the significant and insignificant p-values
# Extract p-value cutoff for E[fdf] < q
fdr <- function(pvals, q, plotit){
  pvals <- pvals[!is.na(pvals)]
  N <- length(pvals)
  
  k <- rank(pvals, ties.method="min")
  alpha <- max(pvals[ pvals <= (q*k/N) ])
  
  if(plotit){
    sig <- factor(pvals <= alpha)
    o <- order(pvals)
    plot(pvals[o], log="xy", col=c("grey60","red")[sig[o]], pch=20, 
      ylab="p-values", xlab="tests ordered by p-value", main = paste('FDR =',q))
    lines(1:N, q*(1:N) / N)
  }
  
}
fdr(p_values, q, plotit=TRUE)
```

