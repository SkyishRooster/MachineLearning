---
title: "03_CrossValidation_Lasso"
author: "Ashley"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
rm(list = ls())

library(readr)
heart <- read_csv("03_heart.csv")
```



### 1. Clean the dataset heart (Dependent variable: heart_attack) and split into training and testing set. Fit a simple linear regression model to predict the heart attack probability and test the model against the test set. 
```{r}
summary(heart)
```

According to the summary table, there are two empty columns: family_record and past_record. Remove them before splitting the data.  
Also, note that there are 2 columns of wrist_dim. The first one has only 2 observations out of 240 while the second one has no missing values. I will remove the first wrist_dim.
```{r}
# Remove empty and nearly empty columns
heart_rm <- heart[, -which(names(heart) %in% c("family_record","past_record", "wrist_dim...15"))]

summary(heart_rm)
```

For the remaining missing values, since there are not many missing values (at most 10, which is less than 5% of the total observations), I simply remove them from the dataset.
```{r}
# Remove rows with missing values
heart_cl <- heart_rm[complete.cases(heart_rm),]
summary(heart_cl)
```

```{r}
# Detect extreme outliers
boxplot(heart_cl)
```

No extreme outliers are detected from the boxplot and the distributions for each column appear sort of balanced. So no action regarding outliers here.

As for strategy for selecting a training subset, since I removed observations that have missing values, there is no treated or untreated concepts here, I would just randomly split the dataset into training and testting sets. If I replaced those missing values with the mean or the mode, then I would need to assign the treated observations proportionally to the training and the testing set.


```{r}
# Randomly split 80% of the dataset into training set
library(caret)

smp_size <- floor(0.8 * nrow(heart_cl))

set.seed(117)
sample <- sample(nrow(heart_cl), size = smp_size)

train <- heart_cl[sample,]
test <- heart_cl[-sample,]
```


```{r}
#Fit the full linear regression model
model_full <- lm(heart_attack ~ ., data = train)
summary(model_full)
```

According to the results of the model, on the one hand, we are 95% confident to say that weight, neck dim, chest dim, abdom dim, hip dim, high dim, ankle dim and biceps dim are positively related to the probability of heart attack, which is also aligned with the commonsense that obesity tends to incur heart attack. On the other hand, height, fat-free weight, and knee dim are negatively related to the probability of heart attack, based on a 95% confidence level. Specifically, we are 99% confident to say that height and fat-free weight are negatively related to the probability of having heart attack.


```{r}
# FDR Control
# Find the adjusted p-value
p_val <- summary(model_full)$coefficients[,4]
p_adj <- p.adjust(p_val, "BH")

# Apply the adjucted p-value to reduce the full model
selector <- p_adj < 0.05
x_full <- c("Intercept", colnames(heart_cl[1:16]))
x_red <- x_full[selector]

# Show the "true" significant variables
model_full$coefficients[selector]
```

According to the results above, after controlling the false discoveries rate, other than biceps dim, previously significant variables remain significant.


```{r}
# Use the model to predict the probability of heart_attack for the test set
y_hat <- predict(model_full, test)

# Calculate the OOS R^2
y_true <- test$heart_attack
y_bar <- mean(y_true)

D0 <- sum((y_true - y_bar) ** 2)
D <- sum((y_true - y_hat) ** 2)

OOS_R <- 1 - (D / D0)
sprintf("The out-of-sample R-squared of the model is %.4f. It implies a good out-of-sample prediction power of the model.", OOS_R)
```


### 2. Explain cross-validation and highlight the problems that may be associated with a cross-validation approach.
To perform cross-validation, we first split the data into K folds. Then fit the model on (K-1) folds at a time and test the model on the remaining fold. Iterate the last step over the K folds. Finally we get K draws of models, and we choose the one with the best OOS performance.  
\
Problems that may come along with cross-validation include:  
- Long computing time due to the repetition;  
- Instability when performing on different samples;  
- Incompatibility with sequential data; 
- Higher likelihood of bias if the data is not representative of the population;  
- Less effectiveness if the data is unbalanced.


### 3. Use only the training sets from (1) and estimate an 8-fold cross-validation to estimate the R^2 of the full model. Calculate the mean R^2 from the 8-fold cross-validation and compare it with the R^2 from (1)
```{r}
# 8-fold cross-validation
library(caret)

cv_ctrl <- trainControl(method = "cv", number = 8)

model_ <- train(heart_attack ~ ., data = train, method = "lm", trControl = cv_ctrl)

# Get the R-squared of each fold
r_sq <- model_$resample$Rsquared
mean_r_sq <- mean(r_sq)

# Compare the mean R-squared and the OOS R-squared from Q1
cbind("OOS R^2" = OOS_R, " R^2 from CV" = mean_r_sq)
```

As we can see from the results, R-squared calculated from cross-validation is higher than the OOS R-squared calculated from typical training and testing data set. It might because when doing cross-validation, the model has seen the validation set, which leads to a higher R-squared value when fitting the validation set. Also, since cross-validation is more robust, the model generated by using this method may inherently better-fit for out-of-sample data.


### 4. Explain Lasso regression and how it work. List the pros and cons associated with using it.
We first set a sequence of penalties from $\lambda_1$, which is the largest that no variable would be able to overcome, to $\lambda_T$, which is the smallest that all varaibles can overcome.  
Then create K folds for the data, and for each fold, fit the lasso path $-\frac{2}{n}logLHD(\beta)+\lambda_t\sum_j|\beta_j|$, where $j\in(1, T)$ on all other folds and get fitted deviance on the data in the left-out fold.  
After that, choose the best $\hat\lambda$ (typically either the minimum or 1 se larger than the min).  
Finally re-fit the model to all the data with the chosen $\hat\lambda$by minimizing $-\frac{2}{n}logLHD(\beta)+\hat{\lambda}\sum_j|\beta_j|$  
\
Pros:  
1. Lasso regression introduces penalties when deciding whether to include a variable or not. As a result, it handles the overfitting issue better than the ordinal regression.   
2. Lasso regression can automatically select the most important variables.  
\
Cons:  
1. Lasso regression cannot do group selection. It hurts when it comes to groups of dummy variables.    
2. Lasso regresstion is sensitive to the scale of input data.


### 5. Fit a Lasso regression to predict the heart attack probability. Use cross-validation to obtain lambda_min as well as lambda_1se. Then compar model outputs from (1), (3), and (5)
```{r}
# Fit a Lasso regression
library(glmnet)
cv_result <- cv.glmnet(as.matrix(train[, 1:16]), train$heart_attack, nfolds = 8, alpha = 1, family = "gaussian", standardize = T)

# Obtain lambda_min and lambda_1se
lambda_min <- cv_result$lambda.min
lambda_1se <- cv_result$lambda.1se

tab <- rbind(cbind(lambda_min, lambda_1se), log(cbind(lambda_min, lambda_1se)))
rownames(tab) <- c("Actual", "Log")
tab
```



```{r}
# Plot the CV Lasso process
plot(cv_result)
```
Aligned with the plot above, lambda_min denotes the lambda that has the minimal OOS MSE (the left line on the graph) whereas lambda_1se denotes the largest lambda (allow fewest variables) that is within 1 standard error of the minimal lambda.  
In this case, I would choose lambda_1se because the OOS MSE of the two models are not very different and lambda_1se allows fewer variables than lambda_min, which meets the principle of parsimony. Besides, it also balances prediction against false discovery and optimistic results of cross-validation.


```{r}
# Output form Q1: model_full, OOS_R
# Output from Q3: model_, mean_r_sq
# Output from Q5: cv_result
lasso_pred <- predict(cv_result, s = lambda_1se, newx = as.matrix(test[,1:16]))
lasso_bar <- mean(lasso_pred)

D0_las <- sum((y_true - lasso_bar) ** 2)
D_las <- sum((y_true - lasso_pred) ** 2)
lasso_R <- 1 - (D_las / D0_las)

# Compare the outputs
output1 <- model_full$coefficients
output3 <- model_$finalModel$coefficients
output5 <- coef(cv_result, select = "1se")

R_tab <- cbind(OOS_R, mean_r_sq, lasso_R)
rownames(R_tab) <- "OOS R-squared"
tab_5 <- rbind(cbind(output1, output3, output5), R_tab)
colnames(tab_5) <- c("Q1", "Q3", "Q5")
tab_5
```

Above is the comparison table of the estimated coefficients and the Out-of-Sample R-squared among the outputs of Q1, Q3, and Q5. As we can see, the R^2 of Q3 (cross-validation without penalties) is highest. The R^2 of lasso regression is slightly lower than the ones of other two models, however, the lasso regressions only include 5 parameters whereas the other two models include 17. So the lasso model actually reduced 70% of the number of variables while only hurt 2.29% OOS R^2 compared to the full model and 7.48% compared to the penalty-free cross-validation model.


### 6. Explain AIC, how it is calculated, and When to use AICc.
AIC is one of the information criteria that approximate distance between a model and "truth".  
\
$AIC=Residual Deviance+2*df$  
\
AICc is corrected AIC by taking the sample size into consideration  
$AICc=Residual Deviance+2*df*\frac{n}{n-df-1}$  
\
Since AICc corrected the flaw of AIC when it comes to a bid data set as well as remained the good performance when the data set is small, it is always better to use AICc than AIC.


