{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM08KN5rQMtB"
      },
      "source": [
        "# 0. Data Retrieval and Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlogI4UWQvIC"
      },
      "source": [
        "## Download data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEvV_VtPN17C"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmpWIOgucdtC"
      },
      "source": [
        "Upload Kaggle JSON file before proceeding ahead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U5Lgi3UOyJC",
        "outputId": "167eabf0-306b-4a27-fb6c-b7f1daf70283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                             title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "thedrcat/daigt-v2-train-dataset                                 DAIGT V2 Train Dataset                            29MB  2023-11-16 01:38:36           1706        174  1.0              \n",
            "derrekdevon/real-estate-sales-2001-2020                         Real Estate Sales 2001-2020                       28MB  2023-12-07 15:36:26           1055         27  1.0              \n",
            "muhammadbinimran/housing-price-prediction-data                  Housing Price Prediction Data                    763KB  2023-11-21 17:56:32           7543        137  1.0              \n",
            "jocelyndumlao/cardiovascular-disease-dataset                    Cardiovascular_Disease_Dataset                   411KB  2023-12-09 06:51:28            991         43  1.0              \n",
            "thedevastator/hotel-bookings-analysis                           Hotel Bookings Analysis                            1MB  2023-12-06 06:17:07            643         25  1.0              \n",
            "thedevastator/netflix-imdb-scores                               Netflix IMDB Scores                              699KB  2023-12-03 14:10:34           2702         46  1.0              \n",
            "andreinovikov/olympic-games                                     Olympic Games                                     13KB  2023-11-15 18:28:55           1645         31  1.0              \n",
            "henryshan/starbucks                                             Starbucks                                          5KB  2023-12-06 03:07:49           1678         41  1.0              \n",
            "thedrcat/daigt-proper-train-dataset                             DAIGT Proper Train Dataset                       119MB  2023-11-05 14:03:25           1682        145  1.0              \n",
            "thedevastator/spotify-tracks-genre-dataset                      Spotify Tracks Genre                               8MB  2023-11-30 04:25:48           2143         59  1.0              \n",
            "carlmcbrideellis/llm-7-prompt-training-dataset                  LLM: 7 prompt training dataset                    41MB  2023-11-15 07:32:56           1649        122  1.0              \n",
            "joebeachcapital/30000-spotify-songs                             30000 Spotify Songs                                3MB  2023-11-01 06:06:43          11669        239  1.0              \n",
            "jacksondivakarr/student-classification-dataset                  Student Classification Dataset                    15KB  2023-12-02 16:23:43           1803         34  1.0              \n",
            "ddosad/auto-sales-data                                          Automobile Sales data                             79KB  2023-11-18 12:36:41           5171         87  1.0              \n",
            "chaudharyanshul/airline-reviews                                 Airline Reviews                                    1MB  2023-11-23 01:44:49           1275         29  1.0              \n",
            "thedevastator/hulu-popular-shows-dataset                        Hulu Popular Shows Dataset                       351KB  2023-12-03 15:03:45            534         21  1.0              \n",
            "nelgiriyewithana/world-educational-data                         World Educational Data                             9KB  2023-11-04 06:10:17           9358        182  1.0              \n",
            "gabrielluizone/high-school-alcoholism-and-academic-performance  High School Alcoholism and Academic Performance  207KB  2023-12-02 17:56:36            916         27  1.0              \n",
            "armanmanteghi/supermarket-sales-dataset-powerbi                 Supermarket Sales Dataset - PowerBI              381KB  2023-12-07 16:31:53            806         27  0.875            \n",
            "fatihilhan/global-superstore-dataset                            Global Superstore Dataset                          3MB  2023-11-16 11:58:36           2274         39  1.0              \n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJnEd3J-OF9u",
        "outputId": "02c7aa5d-e041-4f6a-f6eb-ac5d2407b5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading sentiment140.zip to /content\n",
            " 95% 77.0M/80.9M [00:01<00:00, 70.0MB/s]\n",
            "100% 80.9M/80.9M [00:01<00:00, 61.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3h4QxvzOZKD",
        "outputId": "29781bf8-7fa8-4139-c3cf-2cae2fb465e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  sentiment140.zip\n",
            "  inflating: train/training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ],
      "source": [
        "! mkdir train\n",
        "! unzip sentiment140.zip -d train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJsLEdOmQ5w9"
      },
      "source": [
        "## Set up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db0Z79LAQ9UY"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8bCc3ATQS0q"
      },
      "source": [
        "# 1. EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2YAmKpvdHbQ"
      },
      "source": [
        "## Read the csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VplUPELrQ_Ic"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSYgkWjcRUcX"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsf9DT3Zr1HB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxcO0WHTP4jS",
        "outputId": "a2871852-02cd-4019-d817-606161e67112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- _c1: long (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            "\n",
            "+---+----------+--------------------+--------+---------------+--------------------+\n",
            "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+\n",
            "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
            "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
            "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
            "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
            "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(\"train/training.1600000.processed.noemoticon.csv\", inferSchema=True, encoding='ISO-8859-1')\n",
        "df.printSchema()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts2z1FWvAJtc"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZxTq63slei0",
        "outputId": "e456f38d-5067-4eb3-fa5a-d11cdf66478c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+---+---+---+---+\n",
            "|_c0|_c1|_c2|_c3|_c4|_c5|\n",
            "+---+---+---+---+---+---+\n",
            "+---+---+---+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df['_c3'] != \"NO_QUERY\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H4c3t3Rl3UO"
      },
      "source": [
        "There is no information in _c3 column. Therefore, delete it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcFnzK5jk6xS",
        "outputId": "7b87b7be-4cd1-4b19-c144-c9377fd75851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------------------------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
            "|target|tweet_id  |timestamp                   |user           |text                                                                                                               |\n",
            "+------+----------+----------------------------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
            "|0     |1467810369|Mon Apr 06 22:19:45 PDT 2009|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
            "|0     |1467810672|Mon Apr 06 22:19:49 PDT 2009|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |\n",
            "|0     |1467810917|Mon Apr 06 22:19:53 PDT 2009|mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |\n",
            "|0     |1467811184|Mon Apr 06 22:19:57 PDT 2009|ElleCTF        |my whole body feels itchy and like its on fire                                                                     |\n",
            "|0     |1467811193|Mon Apr 06 22:19:57 PDT 2009|Karoli         |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     |\n",
            "+------+----------+----------------------------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# drop _c3 column\n",
        "df = df.drop('_c3')\n",
        "\n",
        "# rename the columns\n",
        "df = df.withColumnRenamed('_c0', 'target') \\\n",
        "       .withColumnRenamed('_c1', 'tweet_id') \\\n",
        "       .withColumnRenamed('_c2', 'timestamp') \\\n",
        "       .withColumnRenamed('_c4', 'user') \\\n",
        "       .withColumnRenamed('_c5', 'text')\n",
        "\n",
        "df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NOlKLKHa4fz",
        "outputId": "15895c60-dcf7-47c3-c69c-1e417d39009b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------+\n",
            "|time_zone(timestamp)|  count|\n",
            "+--------------------+-------+\n",
            "|                 PDT|1600000|\n",
            "+--------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The timezone name cannot be dealt with directly\n",
        "# Explore the distribution of timezone\n",
        "\n",
        "@udf\n",
        "def time_zone(s):\n",
        "  lst = s.split()\n",
        "  return lst[-2]\n",
        "\n",
        "df.groupby(time_zone('timestamp')).count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jnZwpcJdHSZ"
      },
      "source": [
        "All the tweets were recorded in the PDT time zone. Therefore, we can simply extract the time part without manipulating the time differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm_fwexvnOUr",
        "outputId": "5f221204-1d21-42b9-f5b4-9265750f2999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+--------------------+---------------+--------------------+-------------------+\n",
            "|target|  tweet_id|           timestamp|           user|                text|           datetime|\n",
            "+------+----------+--------------------+---------------+--------------------+-------------------+\n",
            "|     0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|2009-04-06 22:19:45|\n",
            "|     0|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|2009-04-06 22:19:49|\n",
            "|     0|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|2009-04-06 22:19:53|\n",
            "|     0|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|2009-04-06 22:19:57|\n",
            "|     0|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|2009-04-06 22:19:57|\n",
            "+------+----------+--------------------+---------------+--------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# convert string to date\n",
        "from time import strftime, strptime\n",
        "\n",
        "@udf\n",
        "def udf_to_date(s):\n",
        "  lst = s.split()\n",
        "  s = \" \".join(lst[1:4]+[lst[-1]])\n",
        "  return strftime(\"%Y-%m-%d %H:%M:%S\", strptime(s, \"%b %d %H:%M:%S %Y\"))\n",
        "\n",
        "df = df.withColumn('datetime', to_timestamp(udf_to_date('timestamp')))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gOrE8VwE7Um"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0adxAk54zMwI",
        "outputId": "809d2a4d-73d0-430e-8667-1187e7618404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- target: integer (nullable = true)\n",
            " |-- tweet_id: long (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- datetime: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "54JdCgn9mqZM",
        "outputId": "6727c03c-345e-438c-a684-513350d2cfd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Tweets: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1600000"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Total tweets\n",
        "print(\"Total Tweets: \", sep='')\n",
        "display(df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YocqfePQD2Ym",
        "outputId": "6b3b7d6e-4897-46ee-f2a0-ae3a2d8a9583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+---+------+\n",
            "|year|month|day| count|\n",
            "+----+-----+---+------+\n",
            "|2009|    4|  6|  3360|\n",
            "|2009|    4|  7| 17311|\n",
            "|2009|    4| 17|  3084|\n",
            "|2009|    4| 18| 21754|\n",
            "|2009|    4| 19| 27469|\n",
            "|2009|    4| 20| 18460|\n",
            "|2009|    4| 21|  8587|\n",
            "|2009|    5|  1|  7716|\n",
            "|2009|    5|  2| 27434|\n",
            "|2009|    5|  3| 35333|\n",
            "|2009|    5|  4| 15481|\n",
            "|2009|    5|  9| 11739|\n",
            "|2009|    5| 10| 26029|\n",
            "|2009|    5| 11|  4186|\n",
            "|2009|    5| 13|  4066|\n",
            "|2009|    5| 14| 17460|\n",
            "|2009|    5| 16|  9146|\n",
            "|2009|    5| 17| 40154|\n",
            "|2009|    5| 18| 36469|\n",
            "|2009|    5| 21|  2132|\n",
            "|2009|    5| 22| 39074|\n",
            "|2009|    5| 23|   169|\n",
            "|2009|    5| 25|   169|\n",
            "|2009|    5| 26| 10778|\n",
            "|2009|    5| 27|   841|\n",
            "|2009|    5| 28| 15903|\n",
            "|2009|    5| 29| 73827|\n",
            "|2009|    5| 30|103673|\n",
            "|2009|    5| 31| 94588|\n",
            "|2009|    6|  1|110290|\n",
            "|2009|    6|  2| 64192|\n",
            "|2009|    6|  3| 41588|\n",
            "|2009|    6|  4|  7842|\n",
            "|2009|    6|  5| 58757|\n",
            "|2009|    6|  6|111676|\n",
            "|2009|    6|  7| 96350|\n",
            "|2009|    6| 14|  8272|\n",
            "|2009|    6| 15|109781|\n",
            "|2009|    6| 16| 67980|\n",
            "|2009|    6| 17| 44012|\n",
            "|2009|    6| 18| 43004|\n",
            "|2009|    6| 19| 43136|\n",
            "|2009|    6| 20| 45364|\n",
            "|2009|    6| 21| 16360|\n",
            "|2009|    6| 22| 12009|\n",
            "|2009|    6| 23| 17002|\n",
            "|2009|    6| 24|  6299|\n",
            "|2009|    6| 25| 19694|\n",
            "+----+-----+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tweet distribution across date\n",
        "df.groupby(year('datetime').alias('year'),\n",
        "           month('datetime').alias('month'),\n",
        "           dayofmonth('datetime').alias('day')\n",
        "           ).count().orderBy('year', 'month', 'day').show(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0hfZfTSQLG8",
        "outputId": "945bd3fd-8432-4c6e-ac63-ba04b04cf31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of users:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "659775"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Total users\n",
        "print(\"The number of users:\")\n",
        "df.select(df.user).distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V001fhThFFkH",
        "outputId": "8eb4d361-e260-4484-c3e3-e2e4a47d8ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----+\n",
            "|           user|count|\n",
            "+---------------+-----+\n",
            "|       lost_dog|  549|\n",
            "|        webwoke|  345|\n",
            "|       tweetpet|  310|\n",
            "|SallytheShizzle|  281|\n",
            "|    VioletsCRUK|  279|\n",
            "|    mcraddictal|  276|\n",
            "|       tsarnick|  248|\n",
            "|    what_bugs_u|  246|\n",
            "|    Karen230683|  238|\n",
            "|      DarkPiano|  236|\n",
            "+---------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Top 10 active users overview\n",
        "df.groupby('user').count().orderBy('count', ascending=False).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-k_R7P4AVJZ"
      },
      "source": [
        "# 2. NLP Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_0gbe50I7eB",
        "outputId": "c854732d-b478-45bf-8494-06a0ddda6df8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1-YDuIO_bVI"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.pipeline import Transformer\n",
        "\n",
        "class PreprocessTransformer(Transformer):\n",
        "  def __init__(self):\n",
        "      super(PreprocessTransformer, self).__init__()\n",
        "\n",
        "  def _transform(self, df):\n",
        "    # Remove URLs\n",
        "    df = df.withColumn('text_no_url', regexp_replace('text', r'http\\S+', ''))\n",
        "    # Remove user mentions\n",
        "    df = df.withColumn('clean_text', regexp_replace('text_no_url', r'@\\w+', ''))\n",
        "    return df\n",
        "\n",
        "class VaderSentimentTransformer(Transformer):\n",
        "  def __init__(self):\n",
        "    super(VaderSentimentTransformer, self).__init__()\n",
        "\n",
        "  def _transform(self, df):\n",
        "    @udf\n",
        "    def udf_sentiment_vader(text):\n",
        "        sia = SentimentIntensityAnalyzer()\n",
        "        sentiment = sia.polarity_scores(text)\n",
        "        return sentiment['compound']\n",
        "    return df.withColumn(\"sentiment_score\", udf_sentiment_vader(df[\"clean_text\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_r_gDDzXrvh",
        "outputId": "9414fce7-e64e-476f-e2b5-928df784aae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------------------------+---------------+-------------------------------------------------------------------------------------------------------------------+-------------------+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+---------------+\n",
            "|target|tweet_id  |timestamp                   |user           |text                                                                                                               |datetime           |text_no_url                                                                                                    |clean_text                                                                                                     |sentiment_score|\n",
            "+------+----------+----------------------------+---------------+-------------------------------------------------------------------------------------------------------------------+-------------------+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+---------------+\n",
            "|0     |1467810369|Mon Apr 06 22:19:45 PDT 2009|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|2009-04-06 22:19:45|@switchfoot  - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D                    |  - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D                               |-0.0173        |\n",
            "|0     |1467810672|Mon Apr 06 22:19:49 PDT 2009|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |2009-04-06 22:19:49|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!|-0.75          |\n",
            "|0     |1467810917|Mon Apr 06 22:19:53 PDT 2009|mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |2009-04-06 22:19:53|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                      | I dived many times for the ball. Managed to save 50%  The rest go out of bounds                               |0.4939         |\n",
            "|0     |1467811184|Mon Apr 06 22:19:57 PDT 2009|ElleCTF        |my whole body feels itchy and like its on fire                                                                     |2009-04-06 22:19:57|my whole body feels itchy and like its on fire                                                                 |my whole body feels itchy and like its on fire                                                                 |-0.25          |\n",
            "|0     |1467811193|Mon Apr 06 22:19:57 PDT 2009|Karoli         |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     |2009-04-06 22:19:57|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. | no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.                 |-0.6597        |\n",
            "|0     |1467811372|Mon Apr 06 22:20:00 PDT 2009|joy_wolf       |@Kwesidei not the whole crew                                                                                       |2009-04-06 22:20:00|@Kwesidei not the whole crew                                                                                   | not the whole crew                                                                                            |0.0            |\n",
            "|0     |1467811592|Mon Apr 06 22:20:03 PDT 2009|mybirch        |Need a hug                                                                                                         |2009-04-06 22:20:03|Need a hug                                                                                                     |Need a hug                                                                                                     |0.4767         |\n",
            "|0     |1467811594|Mon Apr 06 22:20:03 PDT 2009|coZZ           |@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                |2009-04-06 22:20:03|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?            | hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                     |0.745          |\n",
            "|0     |1467811795|Mon Apr 06 22:20:05 PDT 2009|2Hood4Hollywood|@Tatiana_K nope they didn't have it                                                                                |2009-04-06 22:20:05|@Tatiana_K nope they didn't have it                                                                            | nope they didn't have it                                                                                      |0.0            |\n",
            "|0     |1467812025|Mon Apr 06 22:20:09 PDT 2009|mimismo        |@twittera que me muera ?                                                                                           |2009-04-06 22:20:09|@twittera que me muera ?                                                                                       | que me muera ?                                                                                                |0.0            |\n",
            "+------+----------+----------------------------+---------------+-------------------------------------------------------------------------------------------------------------------+-------------------+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline = Pipeline(stages=[PreprocessTransformer(), VaderSentimentTransformer()])\n",
        "\n",
        "model = pipeline.fit(df)\n",
        "result = model.transform(df)\n",
        "\n",
        "result.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg9GLkLwac9s"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPV4Vr5Oavch"
      },
      "source": [
        "## Public Sentiment Trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62GbZt0Mavcv"
      },
      "outputs": [],
      "source": [
        "# Create the result table for SQL\n",
        "result.select(result.datetime, result.user, result.sentiment_score).createOrReplaceTempView(\"result\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZxxeI8Iavcw",
        "outputId": "30260a57-a2d6-4edb-fefd-4db21982b608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-------------------+\n",
            "|hour|avg_sentiment_score|\n",
            "+----+-------------------+\n",
            "|   0|              0.165|\n",
            "|   1|              0.172|\n",
            "|   2|              0.171|\n",
            "|   3|              0.162|\n",
            "|   4|              0.154|\n",
            "|   5|              0.145|\n",
            "|   6|              0.149|\n",
            "|   7|              0.145|\n",
            "|   8|              0.134|\n",
            "|   9|               0.13|\n",
            "|  10|              0.133|\n",
            "|  11|              0.138|\n",
            "|  12|              0.123|\n",
            "|  13|              0.121|\n",
            "|  14|              0.128|\n",
            "|  15|              0.117|\n",
            "|  16|              0.113|\n",
            "|  17|              0.116|\n",
            "|  18|              0.121|\n",
            "|  19|              0.126|\n",
            "|  20|              0.132|\n",
            "|  21|              0.134|\n",
            "|  22|              0.145|\n",
            "|  23|              0.153|\n",
            "+----+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Public sentiment score over hours of day\n",
        "spark.sql(\" \\\n",
        "SELECT \\\n",
        "  HOUR(datetime) AS hour \\\n",
        "  , ROUND(AVG(sentiment_score), 3) AS avg_sentiment_score \\\n",
        "FROM result \\\n",
        "GROUP BY 1 \\\n",
        "ORDER BY 1;\"\\\n",
        "          ).show(24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyuW7pNfXnEt"
      },
      "source": [
        "According to the sentiment score distribution acroos hours, the public sentiment seems to be low in tweets posted in the afternoon and high in the midnight.  <br><br>It may be because afternoon is usually working/school hours when posting tweets or checking social media isn't convenient/allowed. It means tweets that contain relatively more emotions are posted in this time slot regardless of the inconvenience. And people are more likely to post when it comes to complaints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6uZrim9avcw",
        "outputId": "2cab7785-69dd-4fdb-b4cc-69d9a09ee6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|hour|count|\n",
            "+----+-----+\n",
            "|   0|80865|\n",
            "|   1|75268|\n",
            "|   2|73991|\n",
            "|   3|74253|\n",
            "|   4|76995|\n",
            "|   5|78623|\n",
            "|   6|80852|\n",
            "|   7|83654|\n",
            "|   8|76287|\n",
            "|   9|67278|\n",
            "|  10|60689|\n",
            "|  11|61009|\n",
            "|  12|51653|\n",
            "|  13|49689|\n",
            "|  14|50380|\n",
            "|  15|50643|\n",
            "|  16|55720|\n",
            "|  17|51843|\n",
            "|  18|53485|\n",
            "|  19|57722|\n",
            "|  20|57059|\n",
            "|  21|68964|\n",
            "|  22|78328|\n",
            "|  23|84750|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tweet distribution across hour\n",
        "df.groupby(hour('datetime').alias('hour')).count().orderBy('hour').show(24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH4V6UTpZD5X"
      },
      "source": [
        "The distribution of the total tweets across hours seems to fit with the previous inference. However, it may also be because of non-random sampling which cannot be validated for the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4jElZk8yrhb"
      },
      "source": [
        "## User Sentiment by User Activity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6NV9XJNRPAO",
        "outputId": "6da0bcaa-0eb8-4500-f8fc-c8615c34743d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+--------------+\n",
            "|           user|active_day_cnt|\n",
            "+---------------+--------------+\n",
            "|       lost_dog|            38|\n",
            "|          StDAY|            37|\n",
            "|        adlyman|            34|\n",
            "|       judez_xo|            34|\n",
            "|   MyAppleStuff|            33|\n",
            "|torilovesbradie|            33|\n",
            "|        bigenya|            33|\n",
            "|    Karen230683|            32|\n",
            "|   prateekgupta|            32|\n",
            "|    _magic8ball|            31|\n",
            "+---------------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# user active days count\n",
        "df.createOrReplaceTempView(\"df\")\n",
        "spark.sql(\" \\\n",
        "  SELECT \\\n",
        "    user \\\n",
        "    , COUNT(DISTINCT DATE(datetime)) AS active_day_cnt \\\n",
        "  FROM df \\\n",
        "  GROUP BY user \\\n",
        "  ORDER BY 2 DESC \\\n",
        " \").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivt6c48eSO52",
        "outputId": "f52d6316-e431-415b-aed0-fac93c7ce620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of dates covered:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# All dates included\n",
        "print(\"The number of dates covered:\")\n",
        "df.select(to_date('datetime').alias('date')).distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UM_b09gboxJ",
        "outputId": "88c35cf4-be72-40da-e1f6-df3c314aefa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|category| perc|\n",
            "+--------+-----+\n",
            "|    <= 5|94.84|\n",
            "|   <= 10|98.87|\n",
            "|    <= 3|88.65|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the percentages of users being active no more 10 days, 5 days, and 3 days\n",
        "spark.sql(\"\"\"\n",
        "  WITH active_days AS (\n",
        "    SELECT\n",
        "      user\n",
        "      , COUNT(DISTINCT DATE(datetime)) AS active_day_cnt\n",
        "    FROM df\n",
        "    GROUP BY user\n",
        "  )\n",
        "  SELECT\n",
        "    \"<= 10\" AS category\n",
        "    , ROUND(SUM(INT(active_day_cnt <= 10)) / COUNT(user) * 100, 2) AS perc\n",
        "  FROM active_days\n",
        "  UNION\n",
        "  SELECT\n",
        "    \"<= 5\" AS category\n",
        "    , ROUND(SUM(INT(active_day_cnt <= 5)) / COUNT(user) * 100, 2) AS perc\n",
        "  FROM active_days\n",
        "  UNION\n",
        "  SELECT\n",
        "    \"<= 3\" AS category\n",
        "    , ROUND(SUM(INT(active_day_cnt <= 3)) / COUNT(user) * 100, 2) AS perc\n",
        "  FROM active_days;\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA8D819CbBIt"
      },
      "source": [
        "As we presumed, the number of active days for around 95% users are below 5 days and 89% users are below 3 days. However, there is a wide spread of high active users - 38 days, 37 days, 34 days. <br><br>So I choose to categorize active days into a new variable tweet_freq and explore the relationship between tweet_freq and the average sentiment score of the certain user segment.<br><br>\n",
        "Posted tweets in more than 5 separate days is as \"high\", between 3 to 5 is as \"moderate\", and below 3 is as \"normal\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TvBMCFZIz_ka",
        "outputId": "bf867547-b0c4-4037-b5f0-f763a59d55e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------------+\n",
            "|tweet_freq|avg_sentiment_score|\n",
            "+----------+-------------------+\n",
            "|      high|              0.155|\n",
            "|  moderate|              0.137|\n",
            "|    normal|              0.115|\n",
            "+----------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "  WITH active_days AS (\n",
        "    SELECT\n",
        "      user\n",
        "      , AVG(sentiment_score) AS user_sentiment_score\n",
        "      , COUNT(DISTINCT DATE(datetime)) AS active_day_cnt\n",
        "    FROM result\n",
        "    GROUP BY user\n",
        "  )\n",
        "  SELECT\n",
        "    CASE WHEN active_day_cnt <= 3 THEN 'normal'\n",
        "         WHEN active_day_cnt <= 5 THEN 'moderate'\n",
        "         ELSE 'high' END AS tweet_freq\n",
        "    , ROUND(AVG(user_sentiment_score), 3) AS avg_sentiment_score\n",
        "  FROM active_days\n",
        "  GROUP BY 1\n",
        "  ORDER BY 2 DESC\n",
        "    \"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0TTp_0vbPQG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSlvFRRabbuo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "lM08KN5rQMtB"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}